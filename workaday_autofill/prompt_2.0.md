I want to make a chrome extension that will use a local LLM to help me fill out forms, essentially, i want to have a python fast api app, then my chrome extension. My chrome extenstion will send all input boxes for all information and forms and everything to my fast api app in the form of a dictionary of terms where the keys are the html elements and the values are the element lables such as the questions that are asked before the input or the label of thte input like name or your skills, then my fast api app will send a chat message to a locally run llm that runs on ollama to determine what information, provided by the user, stored in a local JSON file, will go into the corresponding data entry field, the fast api app will then return a dictionary of keys that are the html elements and the values are the LLM's answers to what should go in those fields. this way my chrome extension will then be able to determine what type of input each input is and then be able to parse the dictionary keys and values to properly input each piece of information to the box. basically, i want to use the chrome extension by doing what i need to do on an html page, but when i need to have some help on entering some fields, then i will be able to press a button labled "send to LLM" and then those fields will be automatically populated or list items will be automatically selected based on hte responses from the LLM.